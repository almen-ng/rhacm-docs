[#deploying-aws-private-clusters]
= Deploying a private hosted cluster on AWS (Technology Preview)

After you set up the hosted control planes command line interface, `hcp`, and enable the `local-cluster` as the hosting cluster, you can deploy a hosted cluster or a private hosted cluster on AWS. To deploy a public hosted cluster on AWS, see _Deploying a hosted cluster on AWS_.

By default, hosted control plane guest clusters are publicly accessible through public DNS and the default router for the management cluster.

For private clusters on AWS, all communication with the guest cluster occurs over AWS PrivateLink. To configure hosted control planes for private cluster support on AWS, take the following steps.

*Important:* Although public clusters can be created in any region, private clusters can be created only in the region that is specified by `--aws-private-region`.

* <<prerequisites-aws-private-clusters,Prerequisites>>
* <<access-aws-private-hosted-cluster,Accessing a private hosting cluster on AWS>>
* <<additional-resources-private-hosted-cluster-aws,Additional resources>>

[#prerequisites-aws-private-clusters]
== Prerequisites

To enable private hosted clusters for AWS, you must first enable AWS PrivateLink. For more information, see xref:../../clusters/hosted_control_planes/enable_aws_private_link.adoc#hosted-enable-private-link[Enabling AWS PrivateLink].

[#access-aws-private-hosted-cluster]
== Accessing a private hosting cluster on AWS

You can access a private cluster by using a bastion instance.

//lahinson - july 2023 - update hypershift cli command here
. Start a bastion instance by entering the following command:
+
----
hypershift create bastion aws --aws-creds=<aws-creds> --infra-id=<infra-id> --region=<region> --ssh-key-file=<ssh-key>
----

+
Replace `<ssh-key>` with the SSH public key file to connect to the bastion. The default location for the SSH public key file is `~/.ssh/id_rsa.pub`. Replace `<aws-creds>` with the path to your AWS credentials file, for example, `/user/name/.aws/credentials`.

*Note:* The `hypershift` CLI is not available to download. Use the following commands to extract it by using the HyperShift Operator pod present in the `hypershift` namespace. Replace `<hypershift-operator-pod-name>` with your HyperShift Operator pod name.

----
oc project hypershift
oc rsync <hypershift-operator-pod-name>:/usr/bin/hypershift-no-cgo .
mv hypershift-no-cgo hypershift
----

. Find the private IPs of nodes in the cluster node pool by entering the following command:
+
----
aws ec2 describe-instances --filter="Name=tag:kubernetes.io/cluster/<infra-id>,Values=owned" | jq '.Reservations[] | .Instances[] | select(.PublicDnsName=="") | .PrivateIpAddress'
----

. Create a `kubeconfig` file for the cluster that can be copied to a node by entering the following command:
+
----
hcp create kubeconfig > <cluster-kubeconfig>
----

. Enter the following command to SSH into one of the nodes through the bastion by using the IP that is printed from the `create bastion` command:
+
----
ssh -o ProxyCommand="ssh ec2-user@<bastion-ip> -W %h:%p" core@<node-ip>
----

. From the SSH shell, copy the `kubeconfig` file contents to a file on the node by entering the following command:
+
----
mv <path-to-kubeconfig-file> <new-file-name>
----

. Export the kubeconfig file by entering the following command:
+
----
export KUBECONFIG=<path-to-kubeconfig-file>
----

. Observe the guest cluster status by entering the following command:
+
----
oc get clusteroperators clusterversion
----

[#additional-resources-private-hosted-cluster-aws]
== Additional resources

For more information about deploying a public hosted cluster on AWS, see xref:../hosted_control_planes/managing_hosted_aws.adoc#hosted-deploy-cluster-aws[Deploying a hosted cluster on AWS].
