[#dual-stack-hosted-cluster]
= Deploying the hosted cluster for a dual stack network in a disconnected environment

A hosted cluster is an {ocp-short} cluster with its control plane and API endpoint hosted on a management cluster. The hosted cluster includes the control plane and its corresponding data plane.

You can use the {product-title-short} console and command-line interface (CLI) to create a hosted cluster. The following procedures use CLI to modify the manifests and artifacts.

[#dual-stack-hosted-cluster-objects]
== Deploying hosted cluster objects

Typically, the HyperShift Operator creates the hosted control plane namespace. However, you can include all the objects before the HyperShift Operator begins to reconcile the `HostedCluster` object. When the Operator starts the reconciliation process, finds all of the objects in place.

. Create a YAML file for the hosted cluster and hosted control plane namespaces. See the following configuration:

+
[source,yaml]
----
---
apiVersion: v1
kind: Namespace
metadata:
  creationTimestamp: null
  name: <hosted_cluster_namespace> <1>
spec: {}
status: {}
---
apiVersion: v1
kind: Namespace
metadata:
  creationTimestamp: null
  name: <hosted_control_plane_namespace> <2>
spec: {}
status: {}
----

<1> Specify the hosted cluster namespace, for example, `clusters`.
<2> Specify the hosted control plane namespace, for example, `clusters-hosted-dual`.

*Note: The hosted control plane namespace name must follow the format such as `<hosted_cluster_namespace>-<hosted_cluster_name>`. The hosted cluster name is `hosted-dual`.

. Create a YAML file for the config maps and secrets to include in the `HostedCluster` deployment. See the following configuration:

+
[source,yaml]
----
---
apiVersion: v1
data:
  ca-bundle.crt: |
    -----BEGIN CERTIFICATE-----
    -----END CERTIFICATE-----
kind: ConfigMap
metadata:
  name: <config-map-name> <1>
  namespace: <hosted_cluster_namespace> <2>
---
apiVersion: v1
data:
  .dockerconfigjson: xxxxxxxxx
kind: Secret
metadata:
  creationTimestamp: null
  name: <secret_name> <3>
  namespace: <hosted_cluster_namespace> <2>
---
apiVersion: v1
kind: Secret
metadata:
  name: <secret_name> <3>
  namespace: <hosted_cluster_namespace> <2>
stringData:
  id_rsa.pub: <ssh_public_key_content> <4>
---
apiVersion: v1
data:
  key: nTPtVBEt03owkrKhIdmSW8jrWRxU57KO/fnZa8oaG0Y=
kind: Secret
metadata:
  creationTimestamp: null
  name: <secret_name> <3>
  namespace: <hosted_cluster_namespace> <2>
type: Opaque
----

<1> Specify the config map name.
<2> Specify the hosted cluster namespace, for example, `clusters`.
<3> Specify the secret name.
<4> Enter your SSH public key content. The default path for your SSH public key is `~/.ssh/id_rsa.pub`.

. Create a YAML file for the RBAC roles to configure Assisted Service agents and its API group in the hosted control plane namespace. See the following configuration:

+
[source,yaml]
----
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  creationTimestamp: null
  name: <role_name> <1>
  namespace: <hosted_control_plane_namespace> <2>
rules:
- apiGroups:
  - agent-install.openshift.io
  resources:
  - agents
  verbs:
  - '*'
----

<1> Specify the role name.
<2> Specify the hosted control plane namespace, for example, `clusters-hosted-dual`.

. Create a YAML file for the `HostedCluster` object. See the following configuration:

+
[source,yaml]
----
apiVersion: hypershift.openshift.io/v1beta1
kind: HostedCluster
metadata:
  name: <hosted_cluster_name> <1>
  namespace: <hosted_cluster_namespace> <2>
spec:
  additionalTrustBundle:
    name: "user-ca-bundle"
  olmCatalogPlacement: guest
  imageContentSources: <3>
  - source: quay.io/openshift-release-dev/ocp-v4.0-art-dev
    mirrors:
    - registry.<basedomain-name>:5000/openshift/release  <4>
  - source: quay.io/openshift-release-dev/ocp-release
    mirrors:
    - registry.<basedomain-name>:5000/openshift/release-images
  - mirrors:
  ...
  ...
  autoscaling: {}
  controllerAvailabilityPolicy: SingleReplica
  dns:
    baseDomain: <basedomain>
  etcd:
    managed:
      storage:
        persistentVolume:
          size: <volume_size> <5>
        restoreSnapshotURL: null
        type: PersistentVolume
    managementType: Managed
  fips: false
  networking:
    clusterNetwork:
    - cidr: 10.132.0.0/14
    - cidr: fd01::/48
    networkType: OVNKubernetes
    serviceNetwork:
    - cidr: 172.31.0.0/16
    - cidr: fd02::/112
  platform:
    agent:
      agentNamespace: <hosted_control_plane_namespace> <6>
    type: Agent
  pullSecret:
    name: <hosted_pull_secret_name> <7>
  release:
    image: registry.<basedomain>:5000/openshift/release-images:4.x.y-x86_64 <8>
  secretEncryption:
    aescbc:
      activeKey:
        name: <etcd_encryption_key_name> <9>
    type: aescbc
  services:
  - service: APIServer
    servicePublishingStrategy:
      nodePort:
        address: api.<hosted_cluster_name>.<basedomain>
      type: NodePort
  - service: OAuthServer
    servicePublishingStrategy:
      nodePort:
        address: api.<hosted_cluster_name>.<basedomain>
      type: NodePort
  - service: OIDC
    servicePublishingStrategy:
      nodePort:
        address: api.<hosted_cluster_name>.<basedomain>
      type: NodePort
  - service: Konnectivity
    servicePublishingStrategy:
      nodePort:
        address: api.<hosted_cluster_name>.<basedomain>
      type: NodePort
  - service: Ignition
    servicePublishingStrategy:
      nodePort:
        address: api.<hosted_cluster_name>.<basedomain>
      type: NodePort
  sshKey:
    name: <hosted_ssh_key_name> <10>
status:
  controlPlaneEndpoint:
    host: ""
    port: 0
----

+
<1> Specify the hosted cluster name, for example, `hosted-dual`.
<2> Specify the hosted cluster namespace, for example, `clusters`.
<3> Contains mirror references for user workloads within the hosted cluster.
<4> Specify your base domain name, for example, `dns.base.domain.name`.
<5> Specify the etcd volume size, for example, `8Gi`.
<6> Specify the hosted control plane namespace, for example, `clusters-hosted-dual`. The agent namespace must be same as the hosted control plane namespace.
<7> Specify the pull secret name.
<8> Replace `4.x.y` with the supported {ocp-short} version you want to use.
<9> Specify the encryption key name.
<10> Specify the SSH key name.

. Add an annotation in the `HostedCluster` object that points to the HyperShift Operator release in the {ocp-short} release:

.. Obtain the image payload by entering the following command:

+
----
oc adm release info registry.<dns_base_domain>:5000/openshift-release-dev/ocp-release:4.x.y-x86_64 | grep hypershift
----

+
where `<dns_base_domain>` is the DNS base domain name and `4.x.y` is the supported {ocp-short} version you want to use.

.. See the following output:

+
----
hypershift                                     sha256:31149e3e5f8c5e5b5b100ff2d89975cf5f7a73801b2c06c639bf6648766117f8
----

.. By using the {ocp-short} Images namespace, check the digest by entering the following command:

+
----
podman pull registry.<dns-base-domain-name>:5000/openshift-release-dev/ocp-v4.0-art-dev@sha256:31149e3e5f8c5e5b5b100ff2d89975cf5f7a73801b2c06c639bf6648766117f8
----

+
where `<dns_base_domain>` is the DNS base domain name.

.. See the following output:

+
----
podman pull registry.dns.base.domain.name:5000/openshift/release@sha256:31149e3e5f8c5e5b5b100ff2d89975cf5f7a73801b2c06c639bf6648766117f8
Getting image source signatures
Copying blob d8190195889e skipped: already exists
Writing manifest to image destination
3a62961e6ed6edab46d5ec8429ff1f41d6bb68de51271f037c6cb8941a007fde
----

+
*Note:* The release image that is set in the `HostedCluster` object must use the digest rather than the tag. For example, `quay.io/openshift-release-dev/ocp-release@sha256:e3ba11bd1e5e8ea5a0b36a75791c90f29afb0fdbe4125be4e48f69c76a5c47a0`.

. Create all of the objects by applying your YAML file content in the management cluster. Enter the following command:

+
----
oc apply -f <hosted_cluster_nodeport>.yaml
----

+
Replace `<hosted_cluster_nodeport>.yaml` with your file name.

. Verify that all of the pods in the hosted cluster namespace are in the `running` status. Enter the following command:

+
----
oc get pods -n <hosted_cluster_namespace>
----

. Verify that the hosted cluster is available by entering the following command:

+
----
NAMESPACE   NAME         VERSION   KUBECONFIG                PROGRESS   AVAILABLE   PROGRESSING   MESSAGE
clusters    hosted-dual            hosted-admin-kubeconfig   Partial    True          False         The hosted control plane is available
----

Next, create a `NodePool` object.

[#dual-stack-hosted-cluster-node-pools]
== Creating a NodePool object for the hosted cluster

A `NodePool` is a scalable set of worker nodes that is associated with a hosted cluster. `NodePool` machine architectures remain consistent within a specific pool and are independent of the machine architecture of the control plane.

. Create a YAML file for the `NodePool` object. See the following configuration:

+
[source,yaml]
----
apiVersion: hypershift.openshift.io/v1beta1
kind: NodePool
metadata:
  creationTimestamp: null
  name: <node_pool_name> <1>
  namespace: clusters
spec:
  arch: amd64
  clusterName: hosted-dual
  management:
    autoRepair: false <2>
    upgradeType: InPlace <3>
  nodeDrainTimeout: 0s
  platform:
    type: Agent
  release:
    image: registry.dns.base.domain.name:5000/openshift/release-images:4.x.y-x86_64 <4>
  replicas: 0
status:
  replicas: 0 <5>
----

+
<1> Specify the name for the `NodePool` object.
<2> The `autoRepair` field is set to `false` because the node will not be re-created if it is removed.
<3> The `upgradeType` is set to `InPlace`, which indicates that the same bare metal node is reused during an upgrade.
<4> All of the nodes included in this `NodePool` are based on the following {ocp-short} version: `4.x.y-x86_64`. Replace the `dns.base.domain.name` value with your DNS base domain name and the `4.x.y` value with the supported {ocp-short} version you want to use.
<5> The `replicas` value is set to `0` so that you can scale them when needed. It is important to keep the `NodePool` replicas at 0 until all steps are completed.

. Create the `NodePool` object by entering the following command:

+
----
oc apply -f <nodepool_file_name>.yaml
----

. See the output:

+
----
NAMESPACE   NAME          CLUSTER   DESIRED NODES   CURRENT NODES   AUTOSCALING   AUTOREPAIR   VERSION                              UPDATINGVERSION   UPDATINGCONFIG   MESSAGE
clusters    hosted-dual   hosted    0                               False         False        4.x.y-x86_64
----

Next, create an `InfraEnv` resource.

[#dual-stack-infraenv]
== Creating an InfraEnv resource for the hosted cluster

The `InfraEnv` resource is an Assisted Service object that creates the Red Hat Enterprise Linux CoreOS (RHCOS) boot image for the hosted cluster.

. Create a YAML file with the following `InfraEnv` resource configuration:

+
[source,yaml]
----
---
apiVersion: agent-install.openshift.io/v1beta1
kind: InfraEnv
metadata:
  name: hosted-dual
  namespace: clusters-hosted-dual
spec:
  pullSecretRef: <1>
    name: pull-secret
  sshAuthorizedKey: <ssh_public_key_content> <2>
----

+
<1> The `pullSecretRef` refers to the config map reference in the same namespace as the `InfraEnv`, where the pull secret is used.
<2> Enter your public SSH key content. Your SSH public key is placed in the boot image to allow access to the worker nodes as the `core` user.

. Create the `InfraEnv` resource by entering the following command:

+
----
oc apply -f <infraenv_file_name>.yaml
----

. See the following output:

+
----
NAMESPACE              NAME     ISO CREATED AT
clusters-hosted-dual   hosted   2023-09-11T15:14:10Z
----

Next, create worker nodes.

[#dual-stack-hosted-cluster-worker-nodes]
== Creating worker nodes for the hosted cluster

If you are working on a bare metal platform, creating worker nodes is crucial to ensure that the details in the `BareMetalHost` are correctly configured.

If you are working with virtual machines, you can complete the following steps to create empty worker nodes for the Metal3 Operator to consume. To do so, you use the `kcli` tool.

. If this is not your first attempt to create worker nodes, you must first delete your previous setup. To do so, delete the plan by entering the following command:

+
----
kcli delete plan hosted-dual
----

.. When you are prompted to confirm whether you want to delete the plan, type `y`.

.. Confirm that you see a message stating that the plan was deleted.

. Create the virtual machines by entering the following commands:

+
----
kcli create vm -P start=False -P uefi_legacy=true -P plan=hosted-dual -P memory=8192 -P numcpus=16 -P disks=[200,200] -P nets=["{\"name\": \"dual\", \"mac\": \"aa:aa:aa:aa:11:01\"}"] -P uuid=aaaaaaaa-aaaa-aaaa-aaaa-aaaaaaaa1101 -P name=hosted-dual-worker0
----

+
----
kcli create vm -P start=False -P uefi_legacy=true -P plan=hosted-dual -P memory=8192 -P numcpus=16 -P disks=[200,200] -P nets=["{\"name\": \"dual\", \"mac\": \"aa:aa:aa:aa:11:02\"}"] -P uuid=aaaaaaaa-aaaa-aaaa-aaaa-aaaaaaaa1102 -P name=hosted-dual-worker1
----

+
----
kcli create vm -P start=False -P uefi_legacy=true -P plan=hosted-dual -P memory=8192 -P numcpus=16 -P disks=[200,200] -P nets=["{\"name\": \"dual\", \"mac\": \"aa:aa:aa:aa:11:03\"}"] -P uuid=aaaaaaaa-aaaa-aaaa-aaaa-aaaaaaaa1103 -P name=hosted-dual-worker2
----

+
----
systemctl restart ksushy
----

+
where:

* `start=False` means that the virtual machine (VM) will not automatically start upon creation.
* `uefi_legacy=true` means that you will use UEFI legacy boot to ensure compatibility with previous UEFI implementations.
* `plan=hosted-dual` indicates the plan name, which identifies a group of machines as a cluster.
* `memory=8192` and `numcpus=16` are parameters that specify the resources for the VM, including the RAM and CPU.
* `disks=[200,200]` indicates that you are creating two thin-provisioned disks in the VM.
* `nets=[{"name": "dual", "mac": "aa:aa:aa:aa:02:13"}]` are network details, including the network name to connect to and the MAC address of the primary interface.
* `restart ksushy` restarts the `ksushy` tool to ensure that the tool detects the VMs that you added.

. See the resulting output:

+
----
+---------------------+--------+-------------------+----------------------------------------------------+-------------+---------+
|         Name        | Status |         Ip        |                       Source                       |     Plan    | Profile |
+---------------------+--------+-------------------+----------------------------------------------------+-------------+---------+
|    hosted-worker0   |  down  |                   |                                                    | hosted-dual |  kvirt  |
|    hosted-worker1   |  down  |                   |                                                    | hosted-dual |  kvirt  |
|    hosted-worker2   |  down  |                   |                                                    | hosted-dual |  kvirt  |
+---------------------+--------+-------------------+----------------------------------------------------+-------------+---------+
----

Next, create bare metal hosts for the hosted cluster.

[#dual-stack-bmh-hosted-cluster]
== Creating bare metal hosts for the hosted cluster

A _bare metal host_ is an `openshift-machine-api` object that encompasses physical and logical details so that it can be identified by a Metal3 Operator. Those details are associated with other Assisted Service objects, known as _agents_.

*Important:* Before you create the bare metal host and destination nodes, you must create the virtual machines.

To create a bare metal host, complete the following steps:

. Create a YAML file with the following information:

+
*Note:* Because you have at least one secret that holds the bare metal host credentials, you need to create at least two objects for each worker node.

+
[source,yaml]
----
---
apiVersion: v1
kind: Secret
metadata:
  name: <worker_secret_name>
  namespace: clusters-hosted-dual
data:
  password: YWRtaW4=
  username: YWRtaW4=
type: Opaque
---
apiVersion: metal3.io/v1alpha1
kind: BareMetalHost
metadata:
  name: <worker_node_name>
  namespace: clusters-hosted-dual
  labels:
    infraenvs.agent-install.openshift.io: hosted-dual <1>
  annotations:
    inspect.metal3.io: disabled
    bmac.agent-install.openshift.io/hostname: <worker_node_name> <2>
spec:
  automatedCleaningMode: disabled <3>
  bmc:
    disableCertificateVerification: true <4>
    address: redfish-virtualmedia://[192.168.126.1]:9000/redfish/v1/Systems/local/<worker_node_name> <5>
    credentialsName: <worker_secret_name> <6>
  bootMACAddress: aa:aa:aa:aa:02:11 <7>
  online: true <8>
----

+
<1> `infraenvs.agent-install.openshift.io` serves as the link between the Assisted Installer and the `BareMetalHost` objects.
<2> `bmac.agent-install.openshift.io/hostname` represents the node name that is adopted during deployment.
<3> `automatedCleaningMode` prevents the node from being erased by the Metal3 Operator.
<4> `disableCertificateVerification` is set to `true` to bypass certificate validation from the client.
<5> `address` denotes the baseboard management controller (BMC) address of the worker node.
<6> `credentialsName` points to the secret where the user and password credentials are stored.
<7> `bootMACAddress` indicates the interface MAC address that the node starts from.
<8> `online` defines the state of the node after the `BareMetalHost` object is created.

. Deploy the `BareMetalHost` object by entering the following command:

+
----
oc apply -f <file_name>.yaml
----

+
During the process, you can view the following output:

+
* This output indicates that the process is trying to reach the nodes:

+
----
NAMESPACE         NAME             STATE         CONSUMER   ONLINE   ERROR   AGE
clusters-hosted   hosted-worker0   registering              true             2s
clusters-hosted   hosted-worker1   registering              true             2s
clusters-hosted   hosted-worker2   registering              true             2s
----

+
* This output indicates that the nodes are starting:

+
----
NAMESPACE         NAME             STATE          CONSUMER   ONLINE   ERROR   AGE
clusters-hosted   hosted-worker0   provisioning              true             16s
clusters-hosted   hosted-worker1   provisioning              true             16s
clusters-hosted   hosted-worker2   provisioning              true             16s
----

+
* This output indicates that the nodes started successfully:

+
----
NAMESPACE         NAME             STATE         CONSUMER   ONLINE   ERROR   AGE
clusters-hosted   hosted-worker0   provisioned              true             67s
clusters-hosted   hosted-worker1   provisioned              true             67s
clusters-hosted   hosted-worker2   provisioned              true             67s
----

. After the nodes start, notice the agents in the namespace, as shown in this example:

+
----
NAMESPACE         NAME                                   CLUSTER   APPROVED   ROLE          STAGE
clusters-hosted   aaaaaaaa-aaaa-aaaa-aaaa-aaaaaaaa0411             true       auto-assign
clusters-hosted   aaaaaaaa-aaaa-aaaa-aaaa-aaaaaaaa0412             true       auto-assign
clusters-hosted   aaaaaaaa-aaaa-aaaa-aaaa-aaaaaaaa0413             true       auto-assign
----

+
The agents represent nodes that are available for installation. To assign the nodes to a hosted cluster, scale up the node pool.

[#dual-stack-scale-node-pool-hosted-cluster]
== Scaling up the node pool

After you create the bare metal hosts, their statuses change from `Registering` to `Provisioning` to `Provisioned`. The nodes start with the `LiveISO` of the agent and a default pod that is named `agent`. That agent is responsible for receiving instructions from the Assisted Service Operator to install the {ocp-short} payload.

. To scale up the node pool, enter the following command:

+
----
oc -n <hosted-cluster-namespace> scale nodepool <hosted-cluster-name> --replicas <replica-count>
----

. Verify that the agents are assigned to a hosted cluster by entering the following command:

+
----
oc get agents -n <hosted-control-plane-namespace>
----

+
See the following output:

+
----
NAMESPACE              NAME                                   CLUSTER   APPROVED   ROLE          STAGE
clusters-hosted-dual   aaaaaaaa-aaaa-aaaa-aaaa-aaaaaaaa0411   hosted    true       auto-assign
clusters-hosted-dual   aaaaaaaa-aaaa-aaaa-aaaa-aaaaaaaa0412   hosted    true       auto-assign
clusters-hosted-dual   aaaaaaaa-aaaa-aaaa-aaaa-aaaaaaaa0413   hosted    true       auto-assign
----

. Verify that the node pool replicas are set by entering the following command:

+
----
oc get nodepool -n <hosted-cluster-namespace>
----

+
See the following output:

+
----
NAMESPACE   NAME     CLUSTER   DESIRED NODES   CURRENT NODES   AUTOSCALING   AUTOREPAIR   VERSION                              UPDATINGVERSION   UPDATINGCONFIG   MESSAGE
clusters    hosted   hosted    3                               False         False        4.x.y-x86_64                                      Minimum availability requires 3 replicas, current 0 available
----
+
Replace `4.x.y` with the supported {ocp-short} version that you want to use.

. Wait until the nodes join the cluster.

// command to verify that nodes joined the cluster

Next, monitor the deployment of the hosted cluster.
